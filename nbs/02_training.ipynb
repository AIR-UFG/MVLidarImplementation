{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b023de-9455-4aa6-93ad-09667fca7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, time\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from MVLidarImplementation import data\n",
    "from MVLidarImplementation import model\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce4c78-4f4c-4bf3-b823-1146de445c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "INIT_LR = 0.0001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 4\n",
    "N_CLASSES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21093c-3fb6-4cd0-b1d6-b9bd74a519cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff5257-69ed-4752-894f-15dc339cba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# insert the path to your dataset\n",
    "train_path = Path('../train')\n",
    "test_path = Path(\"../test\")\n",
    "\n",
    "data_paths = [(train_path, \"../train-merged\"), (test_path, \"../test-merged\")]\n",
    "\n",
    "for data_path, merged_path in data_paths:\n",
    "    merged_dir_path = Path(merged_path)\n",
    "    os.makedirs(merged_dir_path, exist_ok=True)\n",
    "    data.merge_images(Path(data_path), merged_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8dff9c-b4af-4be9-b835-4a3871338952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "masks_paths = [(\"../train_segmentation_mask\", train_path), (\"../test_segmentation_mask\", test_path)]\n",
    "\n",
    "for masks_path, data_path in masks_paths:\n",
    "    os.makedirs(masks_path, exist_ok=True)\n",
    "    data.remap_segmentation_masks(data_path, Path(masks_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8ff92-2c68-4228-b06e-2612c3d41213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = data.SemanticDataset(image_folder_path=imgs_train_path,\n",
    "                        mask_folder_path=masks_train_path,\n",
    "                        transform=transform)\n",
    "\n",
    "test_dataset = data.SemanticDataset(image_folder_path=imgs_test_path,\n",
    "                        mask_folder_path=masks_test_path,\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f20af2-fbe1-43af-a5cd-a564feec77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "trainLoader = DataLoader(train_dataset, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())\n",
    "\n",
    "testLoader = DataLoader(test_dataset, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eba5db-6727-438d-87bb-23d1d8015d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mvlidar = model.MVLidar(N_CLASSES).to(DEVICE)\n",
    "\n",
    "lossFunc = CrossEntropyLoss()\n",
    "opt = Adam(mvlidar.parameters(), lr=INIT_LR)\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "testSteps = len(test_dataset) // BATCH_SIZE\n",
    "\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a339818-6c36-4944-aabf-f6490de273f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "print(\"Starting to train the network..\")\n",
    "startTime = time.time()\n",
    "\n",
    "for e in tqdm(range(N_EPOCHS)):\n",
    "    mvlidar.train()\n",
    "\n",
    "    totalTrainLoss = 0\n",
    "    totalTestLoss = 0\n",
    "\n",
    "    for (i, (x, y)) in enumerate(trainLoader):\n",
    "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "        pred = mvlidar(x)\n",
    "        loss = lossFunc(pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        totalTrainLoss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mvlidar.eval()\n",
    "\n",
    "        for (x, y) in testLoader:\n",
    "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "            pred = mvlidar(x)\n",
    "            totalTestLoss += lossFunc(pred, y)\n",
    "\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "        avgTrainLoss, avgTestLoss))\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "    endTime - startTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
