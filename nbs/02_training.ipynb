{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b023de-9455-4aa6-93ad-09667fca7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, time\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from MVLidarImplementation import data\n",
    "from MVLidarImplementation import model\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce4c78-4f4c-4bf3-b823-1146de445c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "INIT_LR = 0.0001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 4\n",
    "N_CLASSES = 7\n",
    "MODEL_PATH = \"mvlidar.pth\"\n",
    "PLOT_PATH = \"plot.png\"\n",
    "TEST_PATHS = \"test_paths.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21093c-3fb6-4cd0-b1d6-b9bd74a519cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff5257-69ed-4752-894f-15dc339cba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# insert the path to your dataset\n",
    "train_path = Path('../train')\n",
    "test_path = Path(\"../test\")\n",
    "\n",
    "data_paths = [(train_path, \"../train-merged\"), (test_path, \"../test-merged\")]\n",
    "\n",
    "for data_path, merged_path in data_paths:\n",
    "    merged_dir_path = Path(merged_path)\n",
    "    os.makedirs(merged_dir_path, exist_ok=True)\n",
    "    data.merge_images(Path(data_path), merged_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8dff9c-b4af-4be9-b835-4a3871338952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "masks_paths = [(\"../train_segmentation_mask\", train_path), (\"../test_segmentation_mask\", test_path)]\n",
    "\n",
    "for masks_path, data_path in masks_paths:\n",
    "    os.makedirs(masks_path, exist_ok=True)\n",
    "    data.remap_segmentation_masks(data_path, Path(masks_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8ff92-2c68-4228-b06e-2612c3d41213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = data.SemanticDataset(image_folder_path=imgs_train_path,\n",
    "                        mask_folder_path=masks_train_path,\n",
    "                        transform=transform)\n",
    "\n",
    "test_dataset = data.SemanticDataset(image_folder_path=imgs_test_path,\n",
    "                        mask_folder_path=masks_test_path,\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f20af2-fbe1-43af-a5cd-a564feec77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "trainLoader = DataLoader(train_dataset, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())\n",
    "\n",
    "testLoader = DataLoader(test_dataset, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eba5db-6727-438d-87bb-23d1d8015d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mvlidar = model.MVLidar(N_CLASSES).to(DEVICE)\n",
    "\n",
    "lossFunc = CrossEntropyLoss(reduction='none')\n",
    "opt = Adam(mvlidar.parameters(), lr=INIT_LR)\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "testSteps = len(test_dataset) // BATCH_SIZE\n",
    "\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093ea2b-dcbe-476a-87ac-384f8d3971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_loss_binary_mask(pred, y):\n",
    "  bin_mask_train = (y !=0).int()\n",
    "  loss = lossFunc(pred, y)\n",
    "  loss = loss * bin_mask_train\n",
    "  loss = loss.mean()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a339818-6c36-4944-aabf-f6490de273f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "\tmvlidar.train()\n",
    "\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\n",
    "\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "\t\tpred = mvlidar(x)\n",
    "\t\tloss = apply_loss_binary_mask(pred, y)\n",
    "\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\ttotalTrainLoss += loss\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tmvlidar.eval()\n",
    "\n",
    "\t\tfor (x, y) in testLoader:\n",
    "\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "\t\t\tpred = mvlidar(x)\n",
    "\t\t\tloss = apply_loss_binary_mask(pred, y)\n",
    "\t\t\ttotalTestLoss += loss\n",
    "\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgTestLoss))\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d87bd2-fedf-4ab4-9850-0d8ff0815342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "\n",
    "# serialize the model to disk\n",
    "torch.save(mvlidar, MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
