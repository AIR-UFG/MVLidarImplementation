{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b023de-9455-4aa6-93ad-09667fca7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, time\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from MVLidarImplementation import model\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc0949-c77b-4143-8d32-80f322f0d64f",
   "metadata": {},
   "source": [
    "By default, Jupyter Lab initializes within the nbs folder, so it's a good practice to return to the root directory and save the path to avoid issues with paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f7e89-414e-427e-8436-27db7b5d68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "os.chdir(\"..\")\n",
    "ROOT_PATH = os.getcwd()\n",
    "ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923c104-6154-4ad0-a862-13f88ae8d78f",
   "metadata": {},
   "source": [
    "To work with the AIR libraries, it's necessary to clone the GitHub repositories, navigate to the directory, perform the installation and import, and then return to the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f87f8-aa59-4750-8a78-e008ffd59352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "libraries_path = ROOT_PATH+\"/libraries\"\n",
    "\n",
    "if not os.path.exists(libraries_path):\n",
    "    os.makedirs(libraries_path)\n",
    "\n",
    "os.chdir(libraries_path)\n",
    "\n",
    "#--depth 1 flag load only the last commit since the repositories are still under development.\n",
    "!git clone --depth 1 https://github.com/AIR-UFG/Cloud2DImageConverter.git\n",
    "!git clone --depth 1 https://github.com/AIR-UFG/SemanticKITTI_Tools.git\n",
    "\n",
    "os.chdir(libraries_path+\"/Cloud2DImageConverter\")\n",
    "!pip install -e '.[dev]'\n",
    "from Cloud2DImageConverter import api\n",
    "\n",
    "os.chdir(libraries_path+\"/SemanticKITTI_Tools\")\n",
    "!pip install -e '.[dev]'\n",
    "from SemanticKITTI_Tools import data\n",
    "\n",
    "os.chdir(ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14f2c4-26ac-4427-8da9-b3c21d810a7a",
   "metadata": {},
   "source": [
    "Set config variables for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce4c78-4f4c-4bf3-b823-1146de445c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "INIT_LR = 0.0001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 4\n",
    "N_CLASSES = 7\n",
    "MODEL_PATH = \"mvlidar.pth\"\n",
    "PLOT_PATH = \"plot.png\"\n",
    "TEST_PATHS = \"test_paths.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012bb15-490c-4b1f-9a31-ea2385908b23",
   "metadata": {},
   "source": [
    "Determine the device to be used for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21093c-3fb6-4cd0-b1d6-b9bd74a519cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cdb8dc-ba19-4410-a5e6-b64623a442c5",
   "metadata": {},
   "source": [
    "Get the projected dataset and merge the depth and reflectance images in a 2-channel image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff5257-69ed-4752-894f-15dc339cba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# insert the path to your dataset\n",
    "train_path = ROOT_PATH+\"/train/\"\n",
    "test_path = ROOT_PATH+\"/test/\"\n",
    "\n",
    "train_merged_path = ROOT_PATH+\"/train-merged/\"\n",
    "test_merged_path = ROOT_PATH+\"/test-merged/\"\n",
    "\n",
    "data_paths = [(train_path, train_merged_path), (test_path, test_merged_path)]\n",
    "\n",
    "for data_path, merged_path in data_paths:\n",
    "    os.makedirs(merged_path, exist_ok=True)\n",
    "    api.merge_images(data_path, merged_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48779fc-e396-4d09-974f-9c1147f04456",
   "metadata": {},
   "source": [
    "The MVLidar article only uses 7 classes for detection, so a remapping of the classes IDs is needed, as in:\n",
    "\n",
    "0 - unknown/outlier: every other class\r\n",
    "\r\n",
    "1 - car: 1\r\n",
    "\r\n",
    "2 - truck: 4\r\n",
    "\r\n",
    "3 - person/pedestrians: 6\r\n",
    "\r\n",
    "4 - cyclist: 7, 2\r\n",
    "\r\n",
    "5 - road: 9,\r\n",
    "\r\n",
    "6 - sidewalk: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef05336-6871-4c7f-8214-49967addbb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "train_segmentation_mask = ROOT_PATH+\"/train_segmentation_mask/\"\n",
    "test_segmentation_mask = ROOT_PATH+\"/test_segmentation_mask/\"\n",
    "\n",
    "masks_paths = [(train_path, train_segmentation_mask), (test_path, test_segmentation_mask)]\n",
    "\n",
    "remapping_rules = {\n",
    "  1: 1,\n",
    "  4: 2,\n",
    "  6: 3,\n",
    "  7: 4,\n",
    "  2: 4,\n",
    "  9: 5,\n",
    "  11: 6\n",
    "}\n",
    "\n",
    "for data_path, mask_path in masks_paths:\n",
    "    os.makedirs(mask_path, exist_ok=True)\n",
    "    data.remap_segmentation_masks(data_path, mask_path, remapping_rules=remapping_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d936e42-6e83-4c23-9757-c64878e35fb2",
   "metadata": {},
   "source": [
    "Creates the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8ff92-2c68-4228-b06e-2612c3d41213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = data.SemanticDataset(image_path=train_merged_path,\n",
    "                        mask_path=train_segmentation_mask,\n",
    "                        transform=transform)\n",
    "\n",
    "test_dataset = data.SemanticDataset(image_path=test_merged_path,\n",
    "                        mask_path=test_segmentation_mask,\n",
    "                        transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203a52b-7f9e-4262-90ef-aa1ed9a1480e",
   "metadata": {},
   "source": [
    "Creates the train and test loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f20af2-fbe1-43af-a5cd-a564feec77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "trainLoader = DataLoader(train_dataset, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())\n",
    "\n",
    "testLoader = DataLoader(test_dataset, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
    "\tnum_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d664b-7475-4c68-966b-53605799798a",
   "metadata": {},
   "source": [
    "The model is imported from the 01_model notebook, CrossEntropy is used as loss and Adam as the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eba5db-6727-438d-87bb-23d1d8015d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "mvlidar = model.MVLidar(N_CLASSES).to(DEVICE)\n",
    "\n",
    "# when the reduction parameter is set to none, it means that no aggregation is applied, and a separate loss value for each input sample is returned\n",
    "lossFunc = CrossEntropyLoss(reduction='none')\n",
    "opt = Adam(mvlidar.parameters(), lr=INIT_LR)\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "testSteps = len(test_dataset) // BATCH_SIZE\n",
    "\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efebe69-f266-4bbb-90e7-44ef4a05489e",
   "metadata": {},
   "source": [
    "A binary mask is applied to the loss in order to ensure that the model only focuses on the labeled regions, and not on the black parts of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093ea2b-dcbe-476a-87ac-384f8d3971d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def apply_loss_binary_mask(pred, y):\n",
    "  bin_mask_train = (y !=0).int()\n",
    "  loss = lossFunc(pred, y)\n",
    "  loss = loss * bin_mask_train\n",
    "  loss = loss.mean()\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0348ac0-71ce-4b3e-93bb-bd5e1fc05c25",
   "metadata": {},
   "source": [
    "## Training\n",
    "Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a339818-6c36-4944-aabf-f6490de273f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "\tmvlidar.train()\n",
    "\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\n",
    "\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "\t\tpred = mvlidar(x)\n",
    "\t\tloss = apply_loss_binary_mask(pred, y)\n",
    "\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\n",
    "\t\ttotalTrainLoss += loss\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tmvlidar.eval()\n",
    "\n",
    "\t\tfor (x, y) in testLoader:\n",
    "\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "\t\t\tpred = mvlidar(x)\n",
    "\t\t\tloss = apply_loss_binary_mask(pred, y)\n",
    "\t\t\ttotalTestLoss += loss\n",
    "\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgTestLoss))\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e3fba-1764-4303-90a9-f5e54847f2b1",
   "metadata": {},
   "source": [
    "Plot the training loss, save the image and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d87bd2-fedf-4ab4-9850-0d8ff0815342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "\n",
    "# serialize the model to disk\n",
    "torch.save(mvlidar, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be858f5-e598-4444-b396-69129de0eb53",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Load the saved model and make the predictions using argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3b107-cd05-458a-a7f1-1d9081134c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def make_predictions(model, imagePath, masksPath, transform):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        image = PILImage.open(imagePath)\n",
    "        image = np.array(image).astype(np.float32)\n",
    "\n",
    "        # Ground truth path\n",
    "        filename = imagePath.split(os.path.sep)[-1]\n",
    "        groundTruthPath = os.path.join(MASK_TEST_DATASET_PATH, filename)\n",
    "\n",
    "\n",
    "        gtMask = PILImage.open(groundTruthPath)\n",
    "        gtMask = np.array(gtMask)\n",
    "\n",
    "        '''\n",
    "        Make channel axis to be the leading one;\n",
    "        Add batch dimension;\n",
    "        Create pytorch tensor;\n",
    "        Flash it to current device\n",
    "        '''\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, 0)\n",
    "        image = torch.from_numpy(image).to(DEVICE)\n",
    "\n",
    "        # Prediction\n",
    "        predMask = model(image).squeeze()\n",
    "\n",
    "        argmax = torch.argmax(predMask, dim=0)\n",
    "\n",
    "        # Cloud2DImageConverter api to convert index to the corresponding color\n",
    "        prediction = api.color_matrix(np.array(argmax.cpu()))\n",
    "        prediction = PILImage.fromarray(prediction)\n",
    "\n",
    "        gtMask = api.color_matrix(gtMask)\n",
    "        gtMask = PILImage.fromarray(gtMask)\n",
    "\n",
    "        # Visualization\n",
    "        display(gtMask)\n",
    "        display(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb5ce0-0bc1-4a26-bb43-04f15eb33bec",
   "metadata": {},
   "source": [
    "Retrieve the test images for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020af29-0ed3-489c-a305-47853aaea76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "imagePaths = os.listdir(test_merged_path)\n",
    "imagePaths = imagePaths[:10]\n",
    "path = os.path.join(test_merged_path, imagePaths[0])\n",
    "image = Image.open(path)\n",
    "image_array = np.array(image)\n",
    "\n",
    "mvlidar = torch.load(MODEL_PATH).to(DEVICE)\n",
    "\n",
    "for i in imagePaths:\n",
    "    path = os.path.join(test_merged_path, i)\n",
    "    make_predictions(mvlidar, path, test_segmentation_mask, transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
